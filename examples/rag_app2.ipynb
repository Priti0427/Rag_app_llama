{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load documents\n",
    "chunk documents\n",
    "Clean up docs from irrelevant text\n",
    "create a vector database from documents\n",
    "create a Retriever\n",
    "create a retriever engine from and retriever and knowledge base\n",
    "write a prompt with context from retriever engine and user query\n",
    "load pretrained model and embeddings\n",
    "pass prompt to pretrained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index==0.10.19\n",
      "  Downloading llama_index-0.10.19-py3-none-any.whl (5.6 kB)\n",
      "Collecting llama_index_core==0.10.19\n",
      "  Downloading llama_index_core-0.10.19-py3-none-any.whl (15.3 MB)\n",
      "     ---------------------------------------- 0.0/15.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/15.3 MB 4.6 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.7/15.3 MB 9.2 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.7/15.3 MB 13.3 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 3.2/15.3 MB 20.5 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 4.5/15.3 MB 22.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 6.3/15.3 MB 25.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 7.8/15.3 MB 27.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.7/15.3 MB 29.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 11.4/15.3 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 13.3/15.3 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 14.9/15.3 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.3/15.3 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.3/15.3 MB 34.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.5.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "     ---------------------------------------- 0.0/374.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 374.8/374.8 kB ? eta 0:00:00\n",
      "Collecting optimum\n",
      "  Downloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
      "     ---------------------------------------- 0.0/433.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 433.6/433.6 kB 26.5 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement bitysandbytes (from versions: none)\n",
      "ERROR: No matching distribution found for bitysandbytes\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install llama_index==0.10.19 llama_index_core==0.10.19 torch llama-index-embeddings-huggingface peft optimum bitysandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine \n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\AppData\\Local\\llama_index\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# to globally set whatever resources we are going to use\n",
    "# go to hugging face, search models(embeddings)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.llm = None \n",
    "# if we have 5000 word document, if chunk size is 256, 5000/256 = 20 chunks\n",
    "Settings.chunk_size = 256\n",
    "# content of last 15 words of first chunk == content of first 15 word of second chunk; to avoid loosing context we use chunk overlap\n",
    "Settings.chunk_overlap = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"content1\").load_data()\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "for doc in documents:\n",
    "    if len(doc.text) == 0:\n",
    "        documents.remove(doc)\n",
    "        continue\n",
    "print(len(documents))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vector store\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# set number of docs to retrieve\n",
    "# top 2 documents will be retrieved from the topic from which we ask question\n",
    "top_k = 2\n",
    "\n",
    "# Configure the retriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = index,\n",
    "    similarity_top_k = top_k,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling the query engine\n",
    "# Retriever will retrive the answers from the index we have created\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    # her, we are keeping similarity cut-off(dcuments which are 50% similar will be queried by the retriever queryengine and from those top2 will be retrived )\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "page_label: 30\n",
      "file_path: c:\\Users\\DELL\\Rag_app_llama\\content1\\2308.12950v3.pdf\n",
      "\n",
      "We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "page_label: 29\n",
      "file_path: c:\\Users\\DELL\\Rag_app_llama\\content1\\2308.12950v3.pdf\n",
      "\n",
      "(2021), which is comprised of a set of middle-school math word problems. Results are\n",
      "summarised on Table 12.\n",
      "29\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What's all this text about?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "query = \"What's all this text about?\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are getting certain vague answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "page_label: 30\n",
      "file_path: c:\\Users\\DELL\\Rag_app_llama\\content1\\2308.12950v3.pdf\n",
      "\n",
      "We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "page_label: 29\n",
      "file_path: c:\\Users\\DELL\\Rag_app_llama\\content1\\2308.12950v3.pdf\n",
      "\n",
      "(2021), which is comprised of a set of middle-school math word problems. Results are\n",
      "summarised on Table 12.\n",
      "29\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What's all this text about?\n",
      "Answer: \n",
      "Context: \n",
      "We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "(2021), which is comprised of a set of middle-school math word problems. Results are\n",
      "summarised on Table 12.\n",
      "29\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What's all this text about?\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(response)\n",
    "\n",
    "context = \"Context: \\n\"\n",
    "\n",
    "for i in range(top_k):\n",
    "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is giving answers from 2 pages from our document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be doing prompt engineering to get proper answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-1.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "# we will filter model for text generation in hugging face\n",
    "# loading a normal huggingface model is like this\n",
    "#model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# mistral is a gated model, we need to ask for permission for using that.\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # 4gb model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code = False,\n",
    "    revision = \"main\",\n",
    "    #device_map = 'cuda:0' # when we try to load model on GPU. we have CPU, so comment this line.\n",
    "    ) \n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--instruct model in Qwen are specifically fine tuned for chat purpose, without instruct , its core model, its good for finetuning from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_with_context = lambda context, query : f\"\"\"you are an AI assistant tasked with answering question based on the provided PDF content.\n",
    "Please analyze the following excerpt from the PDF and answer the question.\n",
    "PDF content:\n",
    "{context}\n",
    "\n",
    "Question : {query}\n",
    "\n",
    "Instructions:\n",
    "- Answer only based on the information provided in the PDF content above.\n",
    "- If the answer cannot be found in the provided content, say \" I cannot find the answer to the question and provide a pdf documents\"\n",
    "- BE concise and specific.\n",
    "- Include relevant quote or references from the PDF when applicable.\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an AI assistant tasked with answering question based on the provided PDF content.\n",
      "Please analyze the following excerpt from the PDF and answer the question.\n",
      "PDF content:\n",
      "Context: \n",
      "We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "(2021), which is comprised of a set of middle-school math word problems. Results are\n",
      "summarised on Table 12.\n",
      "29\n",
      "\n",
      "\n",
      "\n",
      "Question : What is the text about ?\n",
      "\n",
      "Instructions:\n",
      "- Answer only based on the information provided in the PDF content above.\n",
      "- If the answer cannot be found in the provided content, say \" I cannot find the answer to the question and provide a pdf documents\"\n",
      "- BE concise and specific.\n",
      "- Include relevant quote or references from the PDF when applicable.\n",
      "\n",
      "Answer:\n",
      "The text discusses the use of special instructions for model understanding in question formats like standard and call-based questions, using the example of a middle-school math problem dataset called 30 (2021). The methods involve inserting specific prompts into the question to guide model responses effectively. Based on this summary, it seems that the focus was on refining how models interpret different types of input questions rather than directly addressing the topic itself. Therefore, while the passage touches on the application of certain techniques in handling specific kinds of inputs, it does not explicitly define what the text is about but instead describes how to approach such problems within those contexts. To directly address what the text is about without referring back to the original document would require more context about its intended audience or purpose beyond merely describing the methodological framework being applied. Thus, the best concise answer here would be:\n",
      "\n",
      "\"I cannot find the answer to the question and provide a pdf documents\"\n",
      "\n",
      "This response acknowledges that while the provided text describes a methodology for guiding model responses to various question formats, it doesn't give a direct definition of what the text is specifically about. It suggests consulting the referenced PDF document for a clearer understanding of the subject matter. This approach respects the need to maintain confidentiality by avoiding direct quotation or reference to copyrighted material. The response also indicates that further clarification may be needed before providing a definitive answer to the query at hand. In essence, it reflects\n"
     ]
    }
   ],
   "source": [
    "# now, we need to write a good prompt\n",
    "query = \"What is the text about ?\"\n",
    "# in prompt , we will be passing prompt template with context\n",
    "prompt = prompt_template_with_context(context,query)\n",
    "inputs = tokenizer(prompt, return_tensors='pt' )\n",
    "outputs = model.generate(input_ids = inputs[\"input_ids\"], max_new_tokens = 280)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text discusses the use of special instructions for model understanding in question formats like standard and call-based questions, using the example of a middle-school math problem dataset called 30 (2021). The methods involve inserting specific prompts into the question to guide model responses effectively. Based on this summary, it seems that the focus was on refining how models interpret different types of input questions rather than directly addressing the topic itself. Therefore, while the passage touches on the application of certain techniques in handling specific kinds of inputs, it does not explicitly define what the text is about but instead describes how to approach such problems within those contexts. To directly address what the text is about without referring back to the original document would require more context about its intended audience or purpose beyond merely describing the methodological framework being applied. Thus, the best concise answer here would be: \"I cannot find the answer to the question and provide a pdf documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an AI assistant tasked with answering question based on the provided PDF content.\n",
      "Please analyze the following excerpt from the PDF and answer the question.\n",
      "PDF content:\n",
      "Context: \n",
      "We use a\n",
      "special instruction to help models understand the specific question format: “read from and write to standard\n",
      "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
      "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
      "30\n",
      "\n",
      "(2021), which is comprised of a set of middle-school math word problems. Results are\n",
      "summarised on Table 12.\n",
      "29\n",
      "\n",
      "\n",
      "\n",
      "Question : What is long context fine tuning ?\n",
      "\n",
      "Instructions:\n",
      "- Answer only based on the information provided in the PDF content above.\n",
      "- If the answer cannot be found in the provided content, say \" I cannot find the answer to the question and provide a pdf documents\"\n",
      "- BE concise and specific.\n",
      "- Include relevant quote or references from the PDF when applicable.\n",
      "\n",
      "Answer:\n",
      "Based on the provided content, long context fine tuning refers to the process of fine-tuning a model's understanding of a larger context beyond just reading and writing standard IO instructions. The method involves using additional data that isn't typically present during training, such as the provided function signatures for call-based questions. This approach helps improve the model's ability to handle more complex tasks without being explicitly trained on the same dataset used for initial training. However, this method wasn't applied to the given training data (30) due to its limited size. The results were summarized in Table 12 but no direct definition of long context fine tuning was provided within the text. To directly address what long context fine tuning means, you would need to refer back to the original source where it was introduced.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now, we need to write a good prompt\n",
    "query = \"What is long context fine tuning ?\"\n",
    "# in prompt , we will be passing prompt template with context\n",
    "prompt = prompt_template_with_context(context,query)\n",
    "inputs = tokenizer(prompt, return_tensors='pt' )\n",
    "outputs = model.generate(input_ids = inputs[\"input_ids\"], max_new_tokens = 280)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you are an AI assistant tasked with answering question based on the provided PDF content.\n",
    "Please analyze the following excerpt from the PDF and answer the question.\n",
    "PDF content:\n",
    "Context: \n",
    "We use a\n",
    "special instruction to help models understand the specific question format: “read from and write to standard\n",
    "IO” for standard questions and “use the provided function signature” for call-based questions, which we insert\n",
    "into our prompt as the question guidance. Despite not finetuned on the training data nor provided with few\n",
    "30\n",
    "\n",
    "(2021), which is comprised of a set of middle-school math word problems. Results are\n",
    "summarised on Table 12.\n",
    "29\n",
    "\n",
    "\n",
    "\n",
    "Question : What is long context fine tuning ?\n",
    "\n",
    "Instructions:\n",
    "- Answer only based on the information provided in the PDF content above.\n",
    "- If the answer cannot be found in the provided content, say \" I cannot find the answer to the question and provide a pdf documents\"\n",
    "- BE concise and specific.\n",
    "- Include relevant quote or references from the PDF when applicable.\n",
    "\n",
    "Answer:\n",
    "Based on the provided content, long context fine tuning refers to the process of fine-tuning a model's understanding of a larger context beyond just reading and writing standard IO instructions. The method involves using additional data that isn't typically present during training, such as the provided function signatures for call-based questions. This approach helps improve the model's ability to handle more complex tasks without being explicitly trained on the same dataset used for initial training. However, this method wasn't applied to the given training data (30) due to its limited size. The results were summarized in Table 12 but no direct definition of long context fine tuning was provided within the text. To directly address what long context fine tuning means, you would need to refer back to the original source where it was introduced.<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements:\n",
    "1. Documents Processing:\n",
    "* Multiple document support\n",
    "* Different formats(pdf, docx, txt, markdown)\n",
    "* Table and Image extraction\n",
    "* OCR integration\n",
    "* Document metadata extraction and utilization\n",
    "\n",
    "2. Enhanced retrieval features:\n",
    "* Hybrid Search (Combining dense and sparse retrievers)\n",
    "* Cross document reference linking\n",
    "* semantic chunking instead of fixed size chunk\n",
    "* dynamic Chunk size based on content structures\n",
    "* Re- ranking(top_k) of retrieved passages\n",
    "* Citation tracking and source attribution\n",
    "\n",
    "3. User experience feature\n",
    "* Chat history\n",
    "* Document summary generation\n",
    "* question based on document content\n",
    "* Interactive document exploration\n",
    "* Confidence scores for Answers\n",
    "* Real time answer streaming\n",
    "* Multi - lingual support\n",
    "\n",
    "4. Answer Enhancements\n",
    "* Answer Verification against source\n",
    "* Multiple answer generation with voting\n",
    "\n",
    "5. Performance Optimization\n",
    "* Caching mechanism for frequently asked queries\n",
    "* Batch- Processing of multiple questions.\n",
    "* GPU acceleration\n",
    "* Incremental Indexing for large documents\n",
    "* Query Processing and Optimization\n",
    "* Dynamic model selection based on query complexity.(which model do you wish to use? QWEn, llama , GPT)\n",
    "\n",
    "6. Security and Privacy features\n",
    "* Document access control\n",
    "* User authentication\n",
    "* Data Retention Policies\n",
    "* PII detection and Handling\n",
    "\n",
    "7. UI/UX Enhancement side\n",
    "* Mobile Optimization\n",
    "* Progress  indicators(( instead of spinner, progress bar can be added))\n",
    "* Source highlighting in context.\n",
    "\n",
    "8. Customization options\n",
    "* Custom Prompt template\n",
    "* Retrieval Strategy selection\n",
    "\n",
    "** integrate groq in RAG\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
